{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed2c9187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeae44e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage(url):\n",
    "    \"\"\"\n",
    "    Utility function to request a URL and return a BeautifulSoup object for parsing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a session to persist certain parameters across requests\n",
    "    session = requests.Session()\n",
    "    # Set headers to mimic a real browser and avoid basic bot detection\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
    "               'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'}\n",
    "    try:\n",
    "        # Send GET request to the target URL\n",
    "        req = session.get(url, headers=headers)\n",
    "    except requests.exceptions.RequestException:\n",
    "        # Return None if there is any network/HTTP error\n",
    "        return None\n",
    "    # Parse the response HTML with BeautifulSoup\n",
    "    bs = BeautifulSoup(req.text, 'html.parser')\n",
    "    return bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ed90c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target the IMDB Top 250 chart page and parse it\n",
    "url = \"https://www.imdb.com/chart/top\"\n",
    "soup = getPage(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c21be2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the embedded JSON-LD script tag that contains structured data about the chart\n",
    "script_tag = soup.find('script', type='application/ld+json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea476288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and parse the JSON-LD content into a Python dict\n",
    "json_data = script_tag.string\n",
    "\n",
    "data = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b1b8455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of movie entries in the Top 250\n",
    "itemListElement = data['itemListElement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9b32d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list of movies with basic fields extracted from JSON-LD\n",
    "movies = []\n",
    "for i,item in enumerate(itemListElement):\n",
    "    movie = {}\n",
    "    movie['movie_ranking'] = i+1  # 1-based ranking\n",
    "    movie['movie_title'] = item['item']['name']\n",
    "    movie['movie_year'] = None  # to be filled later\n",
    "    movie['movie_country'] = None  # to be filled later\n",
    "    movie['movie_rating'] = item['item']['aggregateRating']['ratingValue']\n",
    "    movie['movie_genre'] = item['item']['genre']\n",
    "    movie['movie_imdb_id'] = item['item']['url'].split('/')[4]\n",
    "    movie['movie_url'] = item['item']['url']\n",
    "\n",
    "    movies.append(movie) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e8b36de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Next.js bootstrapped data that contains additional metadata (e.g., release years)\n",
    "script_tag = soup.find('script', id=\"__NEXT_DATA__\", type=\"application/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab7e9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and parse the Next.js data JSON\n",
    "json_data = script_tag.string\n",
    "data = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf3c8443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the chart edges that contain detailed movie metadata\n",
    "edges = data['props']['pageProps']['pageData']['chartTitles']['edges']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3ed2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract release years from the Next.js data and update movies list\n",
    "release_years = []\n",
    "for edge in edges:\n",
    "    release_years.append(edge['node']['releaseYear']['year'])\n",
    "\n",
    "# Match release years with movies and update the movie_year field\n",
    "for movie, year in zip(movies, release_years):\n",
    "    movie['movie_year'] = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ff829ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape country information for each movie by visiting individual movie pages\n",
    "\n",
    "country_list = []\n",
    "for movie in movies:\n",
    "    url = movie['movie_url']\n",
    "    soup = getPage(url)\n",
    "    # Find country links using regex pattern matching\n",
    "    country_tag = soup.find_all('a', href=re.compile(r'country_of_origin='))\n",
    "    country_list.append(country_tag[0].get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18798d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update movies with country information\n",
    "for movie, country in zip(movies, country_list):\n",
    "    movie['movie_country'] = country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13074541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty reviews list for each movie\n",
    "for movie in movies:\n",
    "    movie['reviews'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac72e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_reviews_graphql(movie_id: str, after: None) -> dict:\n",
    "    \"\"\"Fetch reviews from GraphQL API using a simplified payload structure.\"\"\"\n",
    "\n",
    "    # 1. Use dictionary comprehension to simplify Headers\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Referer\": f\"https://www.imdb.com/title/{title_id}/reviews\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Origin\": \"https://www.imdb.com\",\n",
    "    }\n",
    "    \n",
    "    # 2. Simplify Payload structure, embed variables directly\n",
    "    payload = {\n",
    "        \"operationName\": \"TitleReviewsRefine\",\n",
    "        \"variables\": {\n",
    "            \"after\": after,\n",
    "            \"const\": movie_id,\n",
    "            \"first\": 25,\n",
    "            \"locale\": \"en-US\",\n",
    "            \"sort\": {\"by\": \"HELPFULNESS_SCORE\", \"order\": \"DESC\"},\n",
    "            \"filter\": {},  # Empty dictionary placed last\n",
    "        },\n",
    "        # Persisted query section remains unchanged, this is required by the API\n",
    "        \"extensions\": {\n",
    "            \"persistedQuery\": {\n",
    "                \"sha256Hash\": \"d389bc70c27f09c00b663705f0112254e8a7c75cde1cfd30e63a2d98c1080c87\",\n",
    "                \"version\": 1,\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # 3. Send request\n",
    "    url = \"https://caching.graphql.imdb.com/\"\n",
    "    resp = requests.post(url, json=payload, headers=headers, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b72e4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove HTML tags and normalize text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove HTML tags using regex\n",
    "    text = re.sub(r'<[^>]+>', '', str(text))\n",
    "    \n",
    "    # Decode common HTML entities\n",
    "    entities = {'&amp;': '&', '&lt;': '<', '&gt;': '>', '&quot;': '\"', '&#39;': \"'\", '&nbsp;': ' '}\n",
    "    for entity, replacement in entities.items():\n",
    "        text = text.replace(entity, replacement)\n",
    "    \n",
    "    # Normalize whitespace (multiple spaces/tabs/newlines to single space)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88586223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Reviews for Movies: 100%|██████████| 21/21 [05:50<00:00, 16.70s/it]\n"
     ]
    }
   ],
   "source": [
    "# Scrape reviews for movies\n",
    "for movie in tqdm(movies, desc='Scraping Reviews for Movies'):\n",
    "    \n",
    "    title_id = movie['movie_imdb_id']\n",
    "    all_reviews = []\n",
    "    after = None  # Cursor for pagination\n",
    "\n",
    "    # Fetch first 20 pages of reviews per movie\n",
    "    for page in range(20):  # Fetch first 20 pages\n",
    "        data = fetch_reviews_graphql(title_id, after)\n",
    "        reviews = data['data']['title']['reviews']['edges']\n",
    "        # Extract review data and clean text content\n",
    "        for review in reviews:\n",
    "            review_dict = {}\n",
    "            review_dict['review_rating'] = clean_text(review['node']['authorRating'])\n",
    "            review_dict['review_title'] = review['node']['summary']['originalText']\n",
    "            review_dict['review_content'] = clean_text(review['node']['text']['originalText']['plaidHtml'])\n",
    "            all_reviews.append(review_dict)\n",
    "\n",
    "        # Check if there are more pages available\n",
    "        page_info = data['data']['title']['reviews']['pageInfo']\n",
    "        if not page_info['hasNextPage']:\n",
    "            break\n",
    "        after = page_info['endCursor']  # Update cursor for next page\n",
    "    movie['reviews'] = all_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa9af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save movies and reviews data to CSV file\n",
    "def save_to_csv(movies, output_file='data/movies_reviews.csv'):\n",
    "\n",
    "    \"\"\"Save movies and reviews to CSV file.\"\"\"\n",
    "    print(f\"Saving data to {output_file}...\")\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        # Write CSV header row\n",
    "        writer.writerow([\n",
    "            'movie_rank', 'movie_title', 'movie_year', 'movie_country', \n",
    "            'movie_rating', 'movie_genre', 'movie_imdb_id', 'movie_url',\n",
    "            'review_title', 'review_rating', 'review_content'\n",
    "        ])\n",
    "        \n",
    "        # Write data rows (one row per review)\n",
    "        for movie in movies:\n",
    "            if movie['reviews']:\n",
    "                for review in movie['reviews']:\n",
    "                    writer.writerow([\n",
    "                        movie['movie_ranking'], movie['movie_title'], movie['movie_year'], movie['movie_country'],\n",
    "                        movie['movie_rating'], movie['movie_genre'], movie['movie_imdb_id'], movie['movie_url'],\n",
    "                        review['review_title'], review['review_rating'], review['review_content']\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a1de48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to movies_reviews_part3.csv...\n"
     ]
    }
   ],
   "source": [
    "# Execute the save function to export all scraped data to CSV\n",
    "save_to_csv(movies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
